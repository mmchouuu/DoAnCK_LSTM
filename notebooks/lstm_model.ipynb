{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e7f2bc5",
   "metadata": {},
   "source": [
    "## **PH·∫¶N 1 ‚Äì IMPORT + CONFIG**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10221467",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [],
   "source": [
    "# Ch·∫°y trong cmd\n",
    "conda init cmd.exe\n",
    "\n",
    "\n",
    "conda activate\n",
    "(base) C:\\Users\\cchau>\n",
    "\n",
    "conda activate seq2seq\n",
    "(seq2seq) C:\\Users\\cchau>\n",
    "\n",
    "conda create -n seq2seq python=3.9 -y\n",
    "conda activate seq2seq\n",
    "\n",
    "pip install numpy matplotlib tqdm spacy nltk\n",
    "python -m spacy download en_core_web_sm\n",
    "python -m spacy download de_core_news_sm\n",
    "\n",
    "pip install ipykernel\n",
    "python -m ipykernel install --user --name seq2seq --display-name \"Python (seq2seq)\"\n",
    "\n",
    "Python (seq2seq)\n",
    "\n",
    "# Ch·∫°y trong terminal\n",
    "conda create -n seq2seq python=3.9 -y\n",
    "conda activate seq2seq\n",
    "conda activate seq2seq\n",
    "\n",
    "pip install torch==1.13.1 torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n",
    "\n",
    "conda install -c conda-forge spacy\n",
    "python -m spacy download en_core_web_sm\n",
    "python -m spacy download de_core_news_sm\n",
    "conda install -c conda-forge spacy\n",
    "pip install ipykernel\n",
    "python -m ipykernel install --user --name seq2seq --display-name \"Python (seq2seq)\"\n",
    "pip install matplotlib\n",
    "pip install numpy tqdm nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c72a604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading train.en ‚Üê train.lc.norm.tok.en\n",
      "‚úî Saved: data/train.en\n",
      "Downloading train.de ‚Üê train.lc.norm.tok.de\n",
      "‚úî Saved: data/train.de\n",
      "Downloading val.en ‚Üê val.lc.norm.tok.en\n",
      "‚úî Saved: data/val.en\n",
      "Downloading val.de ‚Üê val.lc.norm.tok.de\n",
      "‚úî Saved: data/val.de\n",
      "Downloading test.en ‚Üê test_2016_flickr.lc.norm.tok.en\n",
      "‚úî Saved: data/test.en\n",
      "Downloading test.de ‚Üê test_2016_flickr.lc.norm.tok.de\n",
      "‚úî Saved: data/test.de\n",
      "\n",
      "üéâ DONE ‚Äî Multi30K EN‚ÄìDE ƒë√£ t·∫£i ho√†n t·∫•t!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "# T·∫°o th∆∞ m·ª•c data n·∫øu ch∆∞a c√≥\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "# File EN-DE t·ª´ Multi30K chu·∫©n (task1/tok)\n",
    "files_map = {\n",
    "    \"train.en\": \"train.lc.norm.tok.en\",\n",
    "    \"train.de\": \"train.lc.norm.tok.de\",\n",
    "    \"val.en\":   \"val.lc.norm.tok.en\",\n",
    "    \"val.de\":   \"val.lc.norm.tok.de\",\n",
    "    \"test.en\":  \"test_2016_flickr.lc.norm.tok.en\",\n",
    "    \"test.de\":  \"test_2016_flickr.lc.norm.tok.de\"\n",
    "}\n",
    "\n",
    "base_url = \"https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/tok/\"\n",
    "\n",
    "def download_file(fname, real_name):\n",
    "    url = base_url + real_name\n",
    "    save_path = os.path.join(\"data\", fname)\n",
    "\n",
    "    print(f\"Downloading {fname} ‚Üê {real_name}\")\n",
    "    r = requests.get(url)\n",
    "\n",
    "    if r.status_code == 200:\n",
    "        with open(save_path, \"wb\") as f:\n",
    "            f.write(r.content)\n",
    "        print(f\"‚úî Saved: data/{fname}\")\n",
    "    else:\n",
    "        print(f\"‚ùå Failed ({r.status_code}): {url}\")\n",
    "\n",
    "# T·∫£i file\n",
    "for fname, real_name in files_map.items():\n",
    "    download_file(fname, real_name)\n",
    "\n",
    "print(\"\\nüéâ DONE ‚Äî Multi30K EN‚ÄìDE ƒë√£ t·∫£i ho√†n t·∫•t!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48ed83a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cchau\\AppData\\Local\\Temp\\ipykernel_1308\\664502001.py:30: UserWarning: Failed to initialize NumPy: DLL load failed while importing _multiarray_umath: The specified module could not be found. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_numpy.cpp:77.)\n",
      "  DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# PH·∫¶N 1 ‚Äì IMPORT & CONFIG (EN‚ÄìDE)\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "os.environ[\"TORCHINDUCTOR_DISABLE\"] = \"1\"   # t·∫Øt Inductor\n",
    "os.environ[\"TORCH_COMPILE_DISABLE\"] = \"1\"   # t·∫Øt torch.compile\n",
    "\n",
    "import torch\n",
    "try:\n",
    "    torch._dynamo.config.suppress_errors = True\n",
    "    torch._dynamo.disable()\n",
    "except:\n",
    "    pass\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import spacy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from tqdm import tqdm\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", DEVICE)\n",
    "\n",
    "# Seed c·ªë ƒë·ªãnh\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if DEVICE == \"cuda\":\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "\n",
    "# C√°c token ƒë·∫∑c bi·ªát\n",
    "SOS_TOKEN = \"<sos>\"\n",
    "EOS_TOKEN = \"<eos>\"\n",
    "PAD_TOKEN = \"<pad>\"\n",
    "UNK_TOKEN = \"<unk>\"\n",
    "\n",
    "SPECIAL_TOKENS = [PAD_TOKEN, SOS_TOKEN, EOS_TOKEN, UNK_TOKEN]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fabbb34",
   "metadata": {},
   "source": [
    "## **PH·∫¶N 2 ‚Äì T·∫¢I D·ªÆ LI·ªÜU MULTI30K (EN‚ÄìDE)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44f61ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 29000\n",
      "Val size: 1014\n",
      "Test size: 1000\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# PH·∫¶N 2 ‚Äì LOAD RAW MULTI30K EN‚ÄìDE\n",
    "# ============================================================\n",
    "\n",
    "DATA_DIR = \"data\"\n",
    "\n",
    "def load_lines(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return [line.strip() for line in f.readlines()]\n",
    "\n",
    "# Load English‚ÄìGerman\n",
    "train_en = load_lines(os.path.join(DATA_DIR, \"train.en\"))\n",
    "train_de = load_lines(os.path.join(DATA_DIR, \"train.de\"))\n",
    "\n",
    "val_en   = load_lines(os.path.join(DATA_DIR, \"val.en\"))\n",
    "val_de   = load_lines(os.path.join(DATA_DIR, \"val.de\"))\n",
    "\n",
    "test_en  = load_lines(os.path.join(DATA_DIR, \"test.en\"))\n",
    "test_de  = load_lines(os.path.join(DATA_DIR, \"test.de\"))\n",
    "\n",
    "print(\"Train size:\", len(train_en))\n",
    "print(\"Val size:\", len(val_en))\n",
    "print(\"Test size:\", len(test_en))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a376fc04",
   "metadata": {},
   "source": [
    "## **PH·∫¶N 3 ‚Äì TOKENIZER**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f256168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     - -------------------------------------- 0.5/12.8 MB 5.6 MB/s eta 0:00:03\n",
      "     ----- ---------------------------------- 1.8/12.8 MB 6.3 MB/s eta 0:00:02\n",
      "     --------- ------------------------------ 2.9/12.8 MB 5.6 MB/s eta 0:00:02\n",
      "     ------------ --------------------------- 3.9/12.8 MB 5.5 MB/s eta 0:00:02\n",
      "     ---------------- ----------------------- 5.2/12.8 MB 5.5 MB/s eta 0:00:02\n",
      "     ------------------- -------------------- 6.3/12.8 MB 5.5 MB/s eta 0:00:02\n",
      "     ----------------------- ---------------- 7.6/12.8 MB 5.6 MB/s eta 0:00:01\n",
      "     --------------------------- ------------ 8.7/12.8 MB 5.7 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 10.0/12.8 MB 5.6 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 11.3/12.8 MB 5.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.6/12.8 MB 5.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 12.8/12.8 MB 5.7 MB/s  0:00:02\n",
      "\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "Collecting de-core-news-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.8.0/de_core_news_sm-3.8.0-py3-none-any.whl (14.6 MB)\n",
      "     ---------------------------------------- 0.0/14.6 MB ? eta -:--:--\n",
      "      --------------------------------------- 0.3/14.6 MB ? eta -:--:--\n",
      "     ---- ----------------------------------- 1.6/14.6 MB 5.6 MB/s eta 0:00:03\n",
      "     -------- ------------------------------- 3.1/14.6 MB 6.4 MB/s eta 0:00:02\n",
      "     ----------- ---------------------------- 4.2/14.6 MB 6.3 MB/s eta 0:00:02\n",
      "     -------------- ------------------------- 5.2/14.6 MB 5.8 MB/s eta 0:00:02\n",
      "     ---------------- ----------------------- 6.0/14.6 MB 5.6 MB/s eta 0:00:02\n",
      "     ------------------- -------------------- 7.1/14.6 MB 5.3 MB/s eta 0:00:02\n",
      "     --------------------- ------------------ 7.9/14.6 MB 5.2 MB/s eta 0:00:02\n",
      "     ------------------------ --------------- 8.9/14.6 MB 5.1 MB/s eta 0:00:02\n",
      "     -------------------------- ------------- 9.7/14.6 MB 5.0 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 10.7/14.6 MB 5.0 MB/s eta 0:00:01\n",
      "     -------------------------------- ------- 11.8/14.6 MB 4.9 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 12.6/14.6 MB 4.9 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 13.6/14.6 MB 4.9 MB/s eta 0:00:01\n",
      "     ---------------------------------------  14.4/14.6 MB 4.9 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 14.6/14.6 MB 4.7 MB/s  0:00:03\n",
      "\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('de_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# PH·∫¶N 3 ‚Äì TOKENIZER (EN‚ÄìDE)\n",
    "# ============================================================\n",
    "\n",
    "!python -m spacy download en_core_web_sm\n",
    "!python -m spacy download de_core_news_sm\n",
    "\n",
    "spacy_en = spacy.load(\"en_core_web_sm\")\n",
    "spacy_de = spacy.load(\"de_core_news_sm\")\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return [tok.text.lower() for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "def tokenize_de(text):\n",
    "    return [tok.text.lower() for tok in spacy_de.tokenizer(text)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988e66c2",
   "metadata": {},
   "source": [
    "## **PH·∫¶N 4 ‚Äì BUILD VOCAB (EN‚ÄìDE)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "167d0cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EN vocab size: 9789\n",
      "DE vocab size: 10004\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# PH·∫¶N 4 ‚Äì BUILD VOCAB (English‚ÄìGerman)\n",
    "# ============================================================\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "def build_vocab(texts, tokenizer, max_size=10000):\n",
    "    counter = Counter()\n",
    "    for sent in texts:\n",
    "        counter.update(tokenizer(sent))\n",
    "\n",
    "    vocab = SPECIAL_TOKENS + [w for w, _ in counter.most_common(max_size)]\n",
    "    \n",
    "    word2idx = {w: i for i, w in enumerate(vocab)}\n",
    "    idx2word = {i: w for i, w in enumerate(vocab)}\n",
    "    \n",
    "    return word2idx, idx2word\n",
    "\n",
    "# Vocab English\n",
    "en_word2idx, en_idx2word = build_vocab(train_en, tokenize_en)\n",
    "\n",
    "# Vocab German\n",
    "de_word2idx, de_idx2word = build_vocab(train_de, tokenize_de)\n",
    "\n",
    "PAD_IDX = en_word2idx[PAD_TOKEN]\n",
    "SOS_IDX = en_word2idx[SOS_TOKEN]\n",
    "EOS_IDX = en_word2idx[EOS_TOKEN]\n",
    "UNK_IDX = en_word2idx[UNK_TOKEN]\n",
    "\n",
    "print(\"EN vocab size:\", len(en_word2idx))\n",
    "print(\"DE vocab size:\", len(de_word2idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78f28c4",
   "metadata": {},
   "source": [
    "## **PH·∫¶N 5 ‚Äì Dataset + DataLoader (EN‚ÄìDE)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49adb27e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(454, 16, 16)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# PH·∫¶N 5 ‚Äì DATASET + DATALOADER (EN‚ÄìDE)\n",
    "# ============================================================\n",
    "\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, src_texts, trg_texts, src_tokenizer, trg_tokenizer, \n",
    "                 src_w2i, trg_w2i):\n",
    "        self.src = src_texts\n",
    "        self.trg = trg_texts\n",
    "        self.src_tokenizer = src_tokenizer\n",
    "        self.trg_tokenizer = trg_tokenizer\n",
    "        self.src_w2i = src_w2i\n",
    "        self.trg_w2i = trg_w2i\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src)\n",
    "\n",
    "    def numericalize(self, tokens, w2i):\n",
    "        return [w2i.get(tok, w2i[UNK_TOKEN]) for tok in tokens]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src_sent = self.src[idx]\n",
    "        trg_sent = self.trg[idx]\n",
    "\n",
    "        src_tok = self.src_tokenizer(src_sent)\n",
    "        trg_tok = self.trg_tokenizer(trg_sent)\n",
    "\n",
    "        # add SOS / EOS cho target\n",
    "        trg_tok = [SOS_TOKEN] + trg_tok + [EOS_TOKEN]\n",
    "\n",
    "        src_ids = self.numericalize(src_tok, self.src_w2i)\n",
    "        trg_ids = self.numericalize(trg_tok, self.trg_w2i)\n",
    "\n",
    "        return torch.tensor(src_ids), torch.tensor(trg_ids)\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    src_batch = [item[0] for item in batch]\n",
    "    trg_batch = [item[1] for item in batch]\n",
    "\n",
    "    src_lens = torch.tensor([len(x) for x in src_batch])\n",
    "    trg_lens = torch.tensor([len(x) for x in trg_batch])\n",
    "\n",
    "    # sort theo ƒë·ªô d√†i gi·∫£m d·∫ßn (b·∫Øt bu·ªôc cho pack_padded)\n",
    "    src_lens, idx = src_lens.sort(descending=True)\n",
    "    src_batch = [src_batch[i] for i in idx]\n",
    "    trg_batch = [trg_batch[i] for i in idx]\n",
    "    trg_lens = trg_lens[idx]\n",
    "\n",
    "    src_pad = pad_sequence(src_batch, padding_value=PAD_IDX, batch_first=True)\n",
    "    trg_pad = pad_sequence(trg_batch, padding_value=PAD_IDX, batch_first=True)\n",
    "\n",
    "    return src_pad, trg_pad, src_lens, trg_lens\n",
    "\n",
    "\n",
    "# T·∫°o dataset\n",
    "train_ds = TranslationDataset(train_en, train_de, tokenize_en, tokenize_de,\n",
    "                              en_word2idx, de_word2idx)\n",
    "val_ds = TranslationDataset(val_en, val_de, tokenize_en, tokenize_de,\n",
    "                            en_word2idx, de_word2idx)\n",
    "test_ds = TranslationDataset(test_en, test_de, tokenize_en, tokenize_de,\n",
    "                             en_word2idx, de_word2idx)\n",
    "\n",
    "# DataLoader\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                          collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                        collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                         collate_fn=collate_fn)\n",
    "\n",
    "len(train_loader), len(val_loader), len(test_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de415cf2",
   "metadata": {},
   "source": [
    "## **PH·∫¶N 6 ‚Äì ENCODER (LSTM)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34af7a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PH·∫¶N 6 ‚Äì ENCODER LSTM\n",
    "# ============================================================\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hidden_dim, num_layers, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim, padding_idx=PAD_IDX)\n",
    "        self.lstm = nn.LSTM(emb_dim, hidden_dim, num_layers,\n",
    "                            dropout=dropout, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src, src_lens):\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "\n",
    "        packed = pack_padded_sequence(embedded, src_lens.cpu(),\n",
    "                                      batch_first=True, enforce_sorted=True)\n",
    "\n",
    "        outputs, (hidden, cell) = self.lstm(packed)\n",
    "\n",
    "        return hidden, cell\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db35033f",
   "metadata": {},
   "source": [
    "## **PH·∫¶N 7 ‚Äì DECODER (LSTM)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36c6a7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PH·∫¶N 7 ‚Äì DECODER LSTM\n",
    "# ============================================================\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hidden_dim, num_layers, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim, padding_idx=PAD_IDX)\n",
    "        self.lstm = nn.LSTM(emb_dim, hidden_dim, num_layers,\n",
    "                            dropout=dropout, batch_first=True)\n",
    "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input_token, hidden, cell):\n",
    "        # input_token shape: [batch]\n",
    "        input_token = input_token.unsqueeze(1)  # -> [batch,1]\n",
    "        embedded = self.dropout(self.embedding(input_token))\n",
    "\n",
    "        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
    "\n",
    "        prediction = self.fc_out(output.squeeze(1))\n",
    "        return prediction, hidden, cell\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb9f561",
   "metadata": {},
   "source": [
    "## **H·∫¶N 8 ‚Äì MODEL WRAPPER + TRAIN LOOP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc260d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PH·∫¶N 8 ‚Äì SEQ2SEQ + TRAIN LOOP\n",
    "# ============================================================\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, src, src_lens, trg, teacher_forcing_ratio=0.5):\n",
    "        batch_size, trg_len = trg.size()\n",
    "        output_dim = self.decoder.fc_out.out_features\n",
    "        \n",
    "        outputs = torch.zeros(batch_size, trg_len, output_dim).to(DEVICE)\n",
    "\n",
    "        hidden, cell = self.encoder(src, src_lens)\n",
    "\n",
    "        input_token = trg[:, 0]   # <sos>\n",
    "\n",
    "        for t in range(1, trg_len):\n",
    "            output, hidden, cell = self.decoder(input_token, hidden, cell)\n",
    "            outputs[:, t] = output\n",
    "\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.argmax(1)\n",
    "\n",
    "            input_token = trg[:, t] if teacher_force else top1\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n",
    "# Kh·ªüi t·∫°o model\n",
    "INPUT_DIM = len(en_word2idx)\n",
    "OUTPUT_DIM = len(de_word2idx)\n",
    "EMB_DIM = 256\n",
    "HID_DIM = 512\n",
    "N_LAYERS = 2\n",
    "DROPOUT = 0.3\n",
    "\n",
    "enc = Encoder(INPUT_DIM, EMB_DIM, HID_DIM, N_LAYERS, DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, EMB_DIM, HID_DIM, N_LAYERS, DROPOUT)\n",
    "\n",
    "model = Seq2Seq(enc, dec).to(DEVICE)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42edb2bc",
   "metadata": {},
   "source": [
    "## **TRAINING FUNCTION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9bc77828",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for src, trg, src_lens, _ in loader:\n",
    "        src, trg = src.to(DEVICE), trg.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(src, src_lens, trg)\n",
    "\n",
    "        # reshape for CE\n",
    "        outputs = outputs[:, 1:].reshape(-1, OUTPUT_DIM)\n",
    "        trg = trg[:, 1:].reshape(-1)\n",
    "\n",
    "        loss = criterion(outputs, trg)\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(loader)\n",
    "\n",
    "\n",
    "def evaluate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for src, trg, src_lens, _ in loader:\n",
    "            src, trg = src.to(DEVICE), trg.to(DEVICE)\n",
    "\n",
    "            outputs = model(src, src_lens, trg, teacher_forcing_ratio=0)\n",
    "\n",
    "            outputs = outputs[:, 1:].reshape(-1, OUTPUT_DIM)\n",
    "            trg = trg[:, 1:].reshape(-1)\n",
    "\n",
    "            loss = criterion(outputs, trg)\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410fa929",
   "metadata": {},
   "source": [
    "## **PH·∫¶N 9 ‚Äì TRAIN TO√ÄN B·ªò M√î H√åNH**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6041e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== TRAINING STARTED =====\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 17\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(N_EPOCHS):\n\u001b[0;32m     15\u001b[0m     epoch_start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 17\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m     val_loss \u001b[38;5;241m=\u001b[39m evaluate(model, val_loader, criterion)\n\u001b[0;32m     20\u001b[0m     epoch_end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "Cell \u001b[1;32mIn[10], line 16\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[1;34m(model, loader, optimizer, criterion)\u001b[0m\n\u001b[0;32m     13\u001b[0m trg \u001b[38;5;241m=\u001b[39m trg[:, \u001b[38;5;241m1\u001b[39m:]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     15\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, trg)\n\u001b[1;32m---> 16\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     19\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\cchau\\anaconda3\\envs\\seq2seq\\lib\\site-packages\\torch\\_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    480\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    481\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    486\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    487\u001b[0m     )\n\u001b[1;32m--> 488\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\cchau\\anaconda3\\envs\\seq2seq\\lib\\site-packages\\torch\\autograd\\__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "N_EPOCHS = 10\n",
    "best_val = float(\"inf\")\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "print(\"===== TRAINING STARTED =====\")\n",
    "\n",
    "start_total = time.time()\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    epoch_start = time.time()\n",
    "\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, criterion)\n",
    "    val_loss = evaluate(model, val_loader, criterion)\n",
    "\n",
    "    epoch_end = time.time()\n",
    "    epoch_time = epoch_end - epoch_start\n",
    "    elapsed = epoch_end - start_total\n",
    "    remaining = (N_EPOCHS - (epoch + 1)) * epoch_time\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    print(f\"\\nEpoch {epoch+1}/{N_EPOCHS}\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f}\")\n",
    "    print(f\"  Val Loss:   {val_loss:.4f}\")\n",
    "    print(f\"  Time:       {epoch_time:.2f} sec\")\n",
    "    print(f\"  Elapsed:    {elapsed/60:.2f} min\")\n",
    "    print(f\"  ETA:        {remaining/60:.2f} min\")\n",
    "\n",
    "    if val_loss < best_val:\n",
    "        best_val = val_loss\n",
    "        torch.save(model.state_dict(), \"best_model_en_de.pth\")\n",
    "        print(\"  ‚Üí Saved best model!\")\n",
    "\n",
    "print(\"\\n===== TRAINING FINISHED =====\")\n",
    "print(f\"Total time: {(time.time() - start_total)/60:.2f} min\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ec6c23",
   "metadata": {},
   "source": [
    "## **PH·∫¶N 10 ‚Äì H√ÄM D·ªäCH translate(sentence)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ed3198",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence, max_len=50):\n",
    "    model.eval()\n",
    "\n",
    "    tokens = tokenize_en(sentence)\n",
    "    ids = [en_word2idx.get(tok, UNK_IDX) for tok in tokens]\n",
    "    src_tensor = torch.tensor(ids).unsqueeze(0).to(DEVICE)\n",
    "    src_len = torch.tensor([len(ids)])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        hidden, cell = model.encoder(src_tensor, src_len)\n",
    "\n",
    "    input_token = torch.tensor([de_word2idx[SOS_TOKEN]]).to(DEVICE)\n",
    "\n",
    "    generated = []\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        output, hidden, cell = model.decoder(input_token, hidden, cell)\n",
    "        top1 = output.argmax(1).item()\n",
    "        word = de_idx2word[top1]\n",
    "\n",
    "        if word == EOS_TOKEN:\n",
    "            break\n",
    "\n",
    "        generated.append(word)\n",
    "        input_token = torch.tensor([top1]).to(DEVICE)\n",
    "\n",
    "    return \" \".join(generated)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0ee2c4",
   "metadata": {},
   "source": [
    "## **PH·∫¶N 11 ‚Äì T√çNH BLEU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1597f11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "def evaluate_bleu(model, loader):\n",
    "    refs = []\n",
    "    hyps = []\n",
    "\n",
    "    for src, trg, src_lens, _ in loader:\n",
    "        src = src.to(DEVICE)\n",
    "\n",
    "        for i in range(src.size(0)):\n",
    "            src_ids = src[i][:src_lens[i]].tolist()\n",
    "            src_sentence = \" \".join([en_idx2word[t] for t in src_ids])\n",
    "\n",
    "            pred = translate(src_sentence)\n",
    "            hyp = pred.split()\n",
    "\n",
    "            trg_ids = trg[i].tolist()\n",
    "            ref_sentence = [de_idx2word[t] for t in trg_ids\n",
    "                            if t not in [PAD_IDX, SOS_IDX, EOS_IDX]]\n",
    "\n",
    "            refs.append([ref_sentence])\n",
    "            hyps.append(hyp)\n",
    "\n",
    "    return corpus_bleu(refs, hyps) * 100\n",
    "\n",
    "bleu_test = evaluate_bleu(model, test_loader)\n",
    "print(\"BLEU on test:\", bleu_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae5d62c",
   "metadata": {},
   "source": [
    "## **PH·∫¶N 12 ‚Äì BI·ªÇU ƒê·ªí LOSS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e7794d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(train_losses, label=\"Train\")\n",
    "plt.plot(val_losses, label=\"Validation\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training & Validation Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74c6b3a",
   "metadata": {},
   "source": [
    "## **PH·∫¶N 13 ‚Äì PH√ÇN T√çCH L·ªñI**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300681b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    \"a man with a red hat is riding a bicycle.\",\n",
    "    \"two dogs are running in the field.\",\n",
    "    \"a woman is sitting on the bench reading a book.\"\n",
    "]\n",
    "\n",
    "for s in examples:\n",
    "    print(\"EN:\", s)\n",
    "    print(\"DE:\", translate(s))\n",
    "    print(\"-----\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seq2seq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
